import torch

class UpperBodySkeletonGradCAM:
    """
    Grad-CAM for upper-body H36M skeleton
    - Input: skeleton (B, T, V, 3)
    - Output: joint-level importance (B, V)
    """
    # TODO
    # def __init__(self, model):
    #     self.model = model
    #     self.activations = None   # (B, V, E)
    #     self.gradients = None     # (B, V, E)

    #     # ğŸ”¥ hook åœ¨ã€Œjoint token è¿˜æ²¡è¢«èšåˆã€çš„ä½ç½®
    #     # âš ï¸ å¦‚æœä½ çš„ SkeletonTokenizer å†…éƒ¨åå­—ä¸åŒï¼Œåªæ”¹è¿™ä¸€è¡Œ
    #     target_module = model.skeleton_tokenizer

    #     def forward_hook(module, inp, out):
    #         # out: (B, V, E)
    #         self.activations = out

    #     def backward_hook(module, grad_in, grad_out):
    #         # grad_out[0]: (B, V, E)
    #         self.gradients = grad_out[0]

    #     target_module.register_forward_hook(forward_hook)
    #     target_module.register_full_backward_hook(backward_hook)

    # def __call__(self, inputs, side="left"):
    #     """
    #     Args:
    #         inputs: dict with key "skeleton": (B, T, V, 3)
    #         side: "left" or "right"

    #     Returns:
    #         cam: (B, V) joint importance
    #     """
    #     assert side in ["left", "right"]

    #     self.model.zero_grad()

    #     # ğŸ”¥ æ ¸å¿ƒï¼šå¿…é¡»è®© joints å‚ä¸æ¢¯åº¦
    #     inputs["joints"].requires_grad_(True)

    #     # ---- forward ----
    #     (feat_l, feat_r), _ = self.model(inputs)

    #     # ---- é€‰æ‹©è§£é‡Šç›®æ ‡ ----
    #     # ç”¨ feature L2 èƒ½é‡ï¼Œè€Œä¸æ˜¯ logitï¼ˆé¿å… CosFace å¹²æ‰°ï¼‰
    #     if side == "left":
    #         score = feat_l.norm(p=2, dim=1).sum()
    #     else:
    #         score = feat_r.norm(p=2, dim=1).sum()

    #     # ---- backward ----
    #     score.backward(retain_graph=True)

    #     # ===== æ­£ç¡®çš„ Skeleton Grad-CAM =====
    #     # activations, gradients: (B, V, E)

    #     if self.activations is None or self.gradients is None:
    #         raise RuntimeError("GradCAM hooks did not capture activations / gradients")

    #     # ğŸ”¥ ä¸è¦ meanï¼Œä¸è¦ normï¼Œç›´æ¥ Grad Ã— Act
    #     cam = (self.gradients * self.activations).sum(dim=-1)  # (B, V)

    #     # ReLU
    #     cam = torch.relu(cam)

    #     # Normalize per-sample
    #     cam = cam / (cam.max(dim=1, keepdim=True)[0] + 1e-6)

    #     print(self.gradients.abs().sum(dim=(0,2)))

    #     return cam.detach()
